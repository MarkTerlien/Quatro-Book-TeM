{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title:  Object-based landuse classification\n",
    "author: Mark Terlien\n",
    "date: \"December 2nd, 2022\"\n",
    "format: \n",
    "  html:\n",
    "    cold-fold: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object-based landuse classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[link](https://towardsdatascience.com/object-based-land-cover-classification-with-python-cbe54e9c9e24)\n",
    "\n",
    "An in-depth explanation of GEOBIA can be found [here](https://www.mdpi.com/2072-4292/12/12/2012#). A practical example can be found [here](https://www.mdpi.com/2072-4292/6/7/6111).\n",
    "\n",
    "Aerial images cover the entire globe at various spatial and temporal resolutions. Timely extraction of information from aerial images requires automated analysis to train computers to recognize what the human eye immediately identifies. Object-based image analysis (OBIA) improves processing efficiency by implementing image segmentation algorithms to combine groups of pixels into objects (segments) reducing the amount of information in and image. This article describes how to use open source Python packages to perform image segmentation and land cover classification of an aerial image. Specifically, I will demonstrate the process of geographic object-based image analysis (GeOBIA)to perform supervised land cover classification in 5 steps:\n",
    "\n",
    "1. Image segmentation\n",
    "2. Quantify segment spectral properties\n",
    "3. Groundtruth data\n",
    "4. Land cover classification\n",
    "5. Accuracy assessment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Python modules\n",
    "\n",
    "The first step is to import the modules we are going to use in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from scikit-image, scipy and sklearn\n",
    "import scipy\n",
    "from skimage import exposure\n",
    "from skimage.segmentation import slic\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Imports gdal and ogr\n",
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "from osgeo import gdalnumeric\n",
    "from osgeo import gdal_array\n",
    "\n",
    "# Import rasterio\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import shapes\n",
    "\n",
    "# Import numpy, pandas and geopandas\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image segmentation\n",
    "\n",
    "Image segmentation is a method in which a digital image is broken down into various subgroups called Image segments which helps in reducing the complexity of the image to make further processing or analysis of the image simpler. Segmentation in easy words is assigning labels to pixels. All picture elements or pixels belonging to the same category have a common label assigned to them. \n",
    "\n",
    "Segmentation effectively reduces the number of elements in an image that need to be classified. This may reduce an image with 1 million pixels down to 50,000 segments, which is much more manageable.\n",
    "\n",
    "[Here](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py) you can read more about image segmentation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the imput parameters\n",
    "\n",
    "After importing the modules, we have to define the locations and the names of the folders and files we are going to use and going to create. \n",
    "\n",
    "We are going to use a satellite image from Waalwijk. Run the following codeblock to define the input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with data files\n",
    "data_folder = 'data_files/Waalwijk/' \n",
    "sentinel_folder = data_folder + 'sentinel-2/'\n",
    "\n",
    "# Name shapefile with sample areas \n",
    "sample_areas_vector_file = data_folder + '/samples_areas.shp'\n",
    "\n",
    "# Name of geojson with landuse classes to be used as groundtruth\n",
    "sample_points_vector_file = data_folder + '/sample_points.geojson'\n",
    "\n",
    "# Name of column with landuse class\n",
    "landuse_column = 'landuse'\n",
    "\n",
    "# Name of column with ID of landuse class\n",
    "landuse_id_column = 'landuse_id'\n",
    "\n",
    "# File with relation between landuse class and id\n",
    "landuse_class_lookup_file = data_folder + '/landuse_id_lookup.csv'\n",
    "\n",
    "# Name of geolsjon file for training and testing\n",
    "train_data_vector_file = data_folder + '/train_data.geojson'\n",
    "test_data_vector_file = data_folder + '/test_data.geojson'\n",
    "\n",
    "# Name rasterfile with segments\n",
    "segments_raster_file = data_folder + '/segments.tif'\n",
    "\n",
    "# Name of new GeoTIFF file with all bands\n",
    "sentinel_bands_raster_file = data_folder + 'sentinel_bands.tif'\n",
    "\n",
    "# Name of new GeoTIFF file with groud truth sample areas\n",
    "sample_areas_raster_file = os.path.splitext(sample_areas_vector_file)[0] + '.tif'\n",
    "\n",
    "# Name of new Geotiff file with classified landuse classes\n",
    "classified_landuse_raster_file = data_folder + 'classified_geobia.tif'\n",
    "classified_landuse_vector_file = data_folder + 'classified_geobia.shp'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the satellite images\n",
    "\n",
    "Now we need to collect all the Sentinel-2 bands because they come as individual images one per band. We merge them into one GeoTIFF image with multiple bands and then we plot the individual bands and the a false color composite for a visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find every file in the sentinal directory\n",
    "sentinal_band_paths_array = [os.path.join(sentinel_folder, f) for f in os.listdir(sentinel_folder) if os.path.isfile(os.path.join(sentinel_folder, f))]\n",
    "sentinal_band_paths_array.sort()\n",
    "\n",
    "# Read metadata of first file and assume all other bands are the same\n",
    "with rasterio.open(sentinal_band_paths_array[0]) as sentinal_band_path_handle:\n",
    "    meta = sentinal_band_path_handle.meta\n",
    "\n",
    "# Update metadata to reflect the number of layers\n",
    "meta.update(count = len(sentinal_band_paths_array))\n",
    "\n",
    "# Read each layer and write it to stack\n",
    "with rasterio.open(sentinel_bands_raster_file, 'w', **meta) as sentinel_bands_handle:\n",
    "    for id, layer in enumerate(sentinal_band_paths_array, start=1):\n",
    "        with rasterio.open(layer) as layer_handle:\n",
    "            sentinel_bands_handle.write_band(id, layer_handle.read(1))\n",
    "\n",
    "# Close files\n",
    "sentinal_band_path_handle = None\n",
    "sentinel_bands_handle = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demonstrates segmentation with the SLIC (Simple linear iterative clustering). First, each of the 4 bands (red, blue, green, near-infrared) from the image is read as a `numpy` array with `gdal`. Band data are re-scaled to intensity values (ranging from 0–1). Then segments are created. Segments are saved to a new raster with `gdal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image has 1059682 pixels\n",
      "Reasonable number of segments for SLIC is estimated at 5298\n",
      "Segmentation complete\n",
      "Generated segments stored as data_files/Waalwijk//segments.tif\n"
     ]
    }
   ],
   "source": [
    "# Open rastefile with bands\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "sentinel_bands_handle = gdal.Open(sentinel_bands_raster_file)\n",
    "nbands = sentinel_bands_handle.RasterCount\n",
    "band_data = []\n",
    "\n",
    "# Build stack with all band arrays and rescale between 0 and 1\n",
    "for i in range(1, nbands+1):\n",
    "    band = sentinel_bands_handle.GetRasterBand(i).ReadAsArray()\n",
    "    band_data.append(band)\n",
    "band_data = np.dstack(band_data)\n",
    "img = exposure.rescale_intensity(band_data)\n",
    "\n",
    "# Estimate the number of segments\n",
    "number_of_pixels = sentinel_bands_handle.RasterXSize * sentinel_bands_handle.RasterYSize\n",
    "print('Image has ' + str(number_of_pixels) + ' pixels')\n",
    "max_number_of_segments = number_of_pixels/200\n",
    "print('Reasonable number of segments for SLIC is estimated at ' + str(int(max_number_of_segments)))\n",
    "\n",
    "# Segmentation, different options with quickshift and slic (only use one of the next two lines)\n",
    "segments = slic(img, n_segments=max_number_of_segments, compactness=0.1)\n",
    "print('Segmentation complete')\n",
    " \n",
    "# Save segments to raster\n",
    "segments_fn = segments_raster_file\n",
    "segments_ds = driverTiff.Create(segments_fn, sentinel_bands_handle.RasterXSize, sentinel_bands_handle.RasterYSize,\n",
    "                                1, gdal.GDT_Float32)\n",
    "segments_ds.SetGeoTransform(sentinel_bands_handle.GetGeoTransform())\n",
    "segments_ds.SetProjection(sentinel_bands_handle.GetProjectionRef())\n",
    "segments_ds.GetRasterBand(1).WriteArray(segments)\n",
    "segments_ds = None\n",
    "segments_fn = None\n",
    "\n",
    "# Print name of segments file\n",
    "print('Generated segments stored as ' + str(segments_raster_file))\n",
    "\n",
    "# TO DO: Vectorize segments.tif \n",
    "# TO DO: Plot vectorized segments.shp on top of satellite image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segments from the SLIC algorithm follow the boundaries of image features. Be sure to assess your segments before continuing with classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of SLIC\n",
    "\n",
    "SLIC (Simple Linear Iterative Clustering) is an algorithm for Superpixel generation. A superpixel can be defined as a group of pixels that share common characteristics (like pixel intensity ). They are often used in image classification because:\n",
    "\n",
    "- They carry more information than pixels.\n",
    "- They have a perceptual meaning since pixels belonging to a given superpixel share similar visual properties.\n",
    "- They provide a compact representation of images that can be useful for computationally demanding problems like landuse classification.\n",
    "\n",
    "The SLIC algorithm generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. [Here](https://www.mdpi.com/2072-4292/9/3/243) you can find an explanation of the the SLIC algorithm.\n",
    "\n",
    "https://www.mdpi.com/2072-4292/9/3/243\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantify segment spectral properties\n",
    "\n",
    "Once the image is segmented the spectral properties of each segment must be quantitatively described. Given a number of pixels, the function below calculates descriptive statistics (e.g. mean, max, min, variance) for each band. These are the values that will be used by the random forests algorithm to classify the segments into landuse types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate stats for each segment\n",
    "def segment_features(segment_pixels):\n",
    "    features = []\n",
    "    npixels, nbands = segment_pixels.shape\n",
    "    for b in range(nbands):\n",
    "        stats = scipy.stats.describe(segment_pixels[:, b])\n",
    "        band_stats = list(stats.minmax) + list(stats)[2:]\n",
    "        if npixels == 1:\n",
    "            # In this case the variance = nan, change it 0.0\n",
    "            band_stats[3] = 0.0\n",
    "        features += band_stats\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we loop through each segment, send the pixels from each segment to the `segment_features` function and save the results in a list. See [here](https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85) for an explanation of skewness and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exmple of the statistics of the first band of the last segment\n",
      "Minimum = 672\n",
      "Maximum = 1682\n",
      "Mean  = 1072.9248120300751\n",
      "Variance  = 77400.69127363864\n",
      "Skewness  = 0.6823140129636995\n",
      "Skewness  = -0.7654967278597749\n"
     ]
    }
   ],
   "source": [
    "# Get the unique ID's for the segments\n",
    "segment_ids = np.unique(segments)\n",
    "\n",
    "# Init the lists \n",
    "objects = []\n",
    "object_ids = []\n",
    "\n",
    "# For each segment\n",
    "for id in segment_ids:\n",
    "\n",
    "    # Select pixels with same ID\n",
    "    segment_pixels = img[segments == id]\n",
    "\n",
    "    # Call function to calculate stats for each segment and store stats and ID's in lists\n",
    "    object_features = segment_features(segment_pixels)\n",
    "    objects.append(object_features)\n",
    "    object_ids.append(id)\n",
    "\n",
    "# Print stats for last segment\n",
    "print('Exmple of the statistics of the first band of the last segment')\n",
    "print('Minimum = ' + str(object_features[0]))\n",
    "print('Maximum = ' + str(object_features[1]))\n",
    "print('Mean  = ' + str(object_features[2]))\n",
    "print('Variance  = ' + str(object_features[3]))\n",
    "print('Skewness  = ' + str(object_features[4]))\n",
    "print('Skewness  = ' + str(object_features[5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Groundtruth data\n",
    "\n",
    "This is a supervised classification workflow, so you’ll need to have some groundtruth data describing the landuse types represented in your classification. \n",
    "\n",
    "The landuse truth data need to be split into training and test data sets. The training data set will train the random forests classification algorithm. We will compare the classification results to the test data set to assess classification accuracy.\n",
    "\n",
    "The landuse data is available as a geojson. The code below uses `geopandas` to read the groundtruth data as a geodataframe. Randomly, 70% of the truth observations are assigned to a training data set and the remaining 30% to a testing data set. The training and test data sets are each saved to a new geojson. During this process I also used a lookup table that I created to give names to each land cover class (lines 8–11). This is not necessary, but makes it easier to see what each class represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water' 'cropland' 'buildings' 'sand' 'forest' 'grasland']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Software\\Python311\\Lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "c:\\Software\\Python311\\Lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "# Read shapefile to geopandas geodataframe\n",
    "gdf = gpd.read_file(sample_points_vector_file)\n",
    "\n",
    "# Get names of landuse classes/labels\n",
    "class_names = gdf[landuse_column].unique()\n",
    "print(class_names)\n",
    "\n",
    "# Create a unique id (integer) for each landuse class/label\n",
    "class_ids = np.arange(class_names.size) + 1\n",
    "\n",
    "# Create a pandas data frame of the labels and ids and save to csv\n",
    "df = pd.DataFrame({landuse_column: class_names, landuse_id_column: class_ids})\n",
    "df.to_csv(landuse_class_lookup_file)\n",
    "\n",
    "# Add a new column to geodatafame with the id for each class/label\n",
    "gdf[landuse_id_column] = gdf[landuse_column].map(dict(zip(class_names, class_ids)))\n",
    " \n",
    "# split the truth data into training and test data sets and save each to a new shapefile\n",
    "gdf_train = gdf.sample(frac=0.7)  # 70% of observations assigned to training data (30% to test data)\n",
    "gdf_test = gdf.drop(gdf_train.index)\n",
    "\n",
    "# Save training and test data to shapefiles\n",
    "gdf_train.to_file(train_data_vector_file)\n",
    "gdf_test.to_file(test_data_vector_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert the training data to raster format so each observation point can be associated with an image segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the trainings dataset\n",
    "train_ds = ogr.Open(train_data_vector_file)\n",
    "lyr = train_ds.GetLayer()\n",
    "\n",
    "# Create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', sentinel_bands_handle.RasterXSize, sentinel_bands_handle.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(sentinel_bands_handle.GetGeoTransform())\n",
    "target_ds.SetProjection(sentinel_bands_handle.GetProjection())\n",
    "\n",
    "# Rasterize the training points\n",
    "options = ['ATTRIBUTE=' + str(landuse_id_column)]\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate each training observation with the corresponding image segment. Lines 13–19 ensure that each training observation is associated with only one segment. Because segments include multiple pixels, it is possible that segments represent multiple land cover types. This is why it is important to properly tune your segmentation algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following segments overlap with landuse class 1: [1021 1854 2422 2534 3578 3659 3694]\n",
      "The following segments overlap with landuse class 2: [1527 1523 2182 2151 3020 3080]\n",
      "The following segments overlap with landuse class 3: [ 538  590  834  969  999 1485 1712 2895 2984 3374 3576]\n",
      "The following segments overlap with landuse class 4: [3362 3587 3844 4046 4415 4598]\n",
      "The following segments overlap with landuse class 5: [2595 3010 3305 3624 3701 4056 4385 4469]\n",
      "The following segments overlap with landuse class 6: [1054 1494 1796 1906 1975 1988 1942 2249 2562 3189 3690]\n"
     ]
    }
   ],
   "source": [
    "# Read groundtruth raster into array\n",
    "ground_truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# Get unique values of landuse classes from raster (0 is the background, or no data, value so it is not included) \n",
    "landuse_classes = np.unique(ground_truth)[1:]\n",
    "\n",
    "# For each landuse class record the associated segment IDs from the segments array and store in dictionary\n",
    "segments_per_class = {}\n",
    "for landuse_class in landuse_classes:\n",
    "    segments_of_class = segments[ground_truth == landuse_class]\n",
    "    print('The following segments overlap with landuse class ' + str(landuse_class) + ': ' + str(segments_of_class))\n",
    "    segments_per_class[landuse_class] = set(segments_of_class)\n",
    " \n",
    "# Make sure no segment ID voerlaps with only one class\n",
    "intersection = set()\n",
    "accum = set()\n",
    "for class_segments in segments_per_class.values():\n",
    "    intersection |= accum.intersection(class_segments)\n",
    "    accum |= class_segments\n",
    "assert len(intersection) == 0, \"Segment(s) represent multiple classes\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landuse Classification\n",
    "\n",
    "This is the meat of the analysis. The classification algorithm. First, identify and label the training objects (lines 1–20). This process involves associating a label (land cover type) with the statistics describing each spectral band within the image segment.\n",
    "\n",
    "Now, everything is now set up to train a classifier and use it to predict across all segments in the image. Here I’m using random forests, a popular classification algorithm. The code to train (fit) the algorithm and make predictions is quite simple (lines 22–24). Simply pass the training objects (containing the spectral properties) and the associated land cover label to the classifier. Once the classifier is trained (fitted) predictions can be made for non-training segments based on their spectral properties. After the predictions are made, save them to raster for display in a GIS (lines 26–43)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_img = np.copy(segments)\n",
    "threshold = train_img.max() + 1  # Make the threshold value greater than any landuse class value\n",
    "\n",
    "# All pixels in training segments assigned value greater than threshold\n",
    "for landuse_class in landuse_classes:\n",
    "    class_label = threshold + landuse_class\n",
    "    for segment_id in segments_per_class[landuse_class]:\n",
    "        train_img[train_img == segment_id] = class_label\n",
    " \n",
    "# Training segments receive landuse class value, all other segments 0\n",
    "train_img[train_img <= threshold] = 0\n",
    "train_img[train_img > threshold] -= threshold\n",
    "\n",
    "# Create objects and labels for training data\n",
    "training_objects = []\n",
    "training_labels = []\n",
    "for landuse_class in landuse_classes:\n",
    "    class_train_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class[landuse_class]]\n",
    "    training_labels += [landuse_class] * len(class_train_object)\n",
    "    training_objects += class_train_object\n",
    " \n",
    "classifier = RandomForestClassifier(n_jobs=-1)  # Setup random forest classifier\n",
    "classifier.fit(training_objects, training_labels)  # Fit rf classifier\n",
    "predicted = classifier.predict(objects)  # Predict with rf classifier\n",
    "\n",
    "# Create numpy array from rf classification and save to raster\n",
    "clf = np.copy(segments)\n",
    "for segment_id, klass in zip(segment_ids, predicted):\n",
    "    clf[clf == segment_id] = klass\n",
    "\n",
    "# This section masks no data values \n",
    "mask = np.sum(img, axis=2)  \n",
    "mask[mask > 0.0] = 1.0\n",
    "mask[mask == 0.0] = -1.0\n",
    "clf = np.multiply(clf, mask)\n",
    "clf[clf < 0] = -9999.0\n",
    " \n",
    "clfds = driverTiff.Create(classified_landuse_raster_file, sentinel_bands_handle.RasterXSize, sentinel_bands_handle.RasterYSize,\n",
    "                          1, gdal.GDT_Float32)  # this section saves to raster\n",
    "clfds.SetGeoTransform(sentinel_bands_handle.GetGeoTransform())\n",
    "clfds.SetProjection(sentinel_bands_handle.GetProjection())\n",
    "clfds.GetRasterBand(1).SetNoDataValue(-9999.0)\n",
    "clfds.GetRasterBand(1).WriteArray(clf)\n",
    "clfds = None\n",
    " \n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Assessment with a Confusion Matrix\n",
    "\n",
    "Accuracy assessment is a crucial aspect of any classification. If your classification doesn’t represent what it’s supposed to, it’s not worth much. Because the emphasis of this article is to describe the GeOBIA workflow, I’m not presenting my own accuracy results. Instead, I’ll show how to generate a basic confusion matrix for accuracy assessment.\n",
    "\n",
    "Load the test data set created earlier and convert it to raster format so it is compatible with the generated predictions. Then simply query the predicted values from the locations where test data exist. Finally, generate the confusion matrix from the corresponding values.\n",
    "\n",
    "Note: We classified segments, but this accuracy assessment compares pixels. We’re comparing all the pixels in each test segment to all the pixels in the corresponding predicted segment. This could lead to some bias if certain land cover classes are more frequently found in smaller (or larger) segments than others. Again, if you have done due diligence with image segmentation, this shouldn’t be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gdal\n",
    "import ogr\n",
    "from sklearn import metrics\n",
    " \n",
    "# read original image to get info for raster dimensions\n",
    "naip_fn = 'path/to/image.tif'\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    " \n",
    "# rasterize test data for pixel-to-pixel comparison\n",
    "test_fn = 'path/to/test.shp'\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    " \n",
    "truth = target_ds.GetRasterBand(1).ReadAsArray()  # truth/test data array\n",
    " \n",
    "pred_ds = gdal.Open('path/to/classified_result.tif')  \n",
    "pred = pred_ds.GetRasterBand(1).ReadAsArray()  # predicted data array\n",
    "idx = np.nonzero(truth) # get indices where truth/test has data values\n",
    "cm = metrics.confusion_matrix(truth[idx], pred[idx])  # create a confusion matrix at the truth/test locations\n",
    " \n",
    "# pixel accuracy\n",
    "print(cm)\n",
    "print(cm.diagonal())\n",
    "print(cm.sum(axis=0))\n",
    "accuracy = cm.diagonal() / cm.sum(axis=0)  # overall accuracy\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6090159f195111259c09d95d79fb1f2622e138688553073b14c580eb658dcffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
