{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title:  Object-based landuse classification\n",
    "author: Mark Terlien\n",
    "date: \"December 2nd, 2022\"\n",
    "format: \n",
    "  html:\n",
    "    cold-fold: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object-based landuse classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[link](https://towardsdatascience.com/object-based-land-cover-classification-with-python-cbe54e9c9e24)\n",
    "\n",
    "Aerial images cover the entire globe at various spatial and temporal resolutions. Timely extraction of information from aerial images requires automated analysis to train computers to recognize what the human eye immediately identifies. Object-based image analysis (OBIA) improves processing efficiency by implementing image segmentation algorithms to combine groups of pixels into objects (segments) reducing the amount of information in and image. This article describes how to use open source Python packages to perform image segmentation and land cover classification of an aerial image. Specifically, I will demonstrate the process of geographic object-based image analysis (GeOBIA)to perform supervised land cover classification in 5 steps:\n",
    "\n",
    "- Image segmentation\n",
    "- Quantify segment spectral properties\n",
    "- Truth data\n",
    "- Land cover classification\n",
    "- Accuracy assessment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image segmentation\n",
    "\n",
    "The image above is a portion of an aerial photo collected by the US Department of Agriculture (USDA) under the National Agricultural Imagery Progam (NAIP). The horizontal image resolution is 1 meter. Our first task is to group similar pixels into segments. Segmentation effectively reduces the number of elements in an image that need to be classified. This may reduce an image with 1 million pixels down to 50,000 segments, which is much more manageable.\n",
    "\n",
    "A number of segmentation algorithms are available. We won’t go into details in detail in this article. I will show you results from two different algorithms, and how to implement them in Python with `skimage`.\n",
    "\n",
    "[Here](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py) you can read more about image segmentation.\n",
    "\n",
    "The code below demonstrates segmentation with the SLIC (Simple linear iterative clustering) and quickshift algorithms (lines 23 and 24, respectively). First, each of the 4 bands (red, blue, green, near-infrared) from the NAIP image is read as a `numpy` array with `gdal`. Band data are re-scaled to intensity values (ranging from 0–1). Then segments are created. Segments are saved to a new raster with `gdal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "from skimage import exposure\n",
    "from skimage.segmentation import quickshift\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    " \n",
    "naip_fn = 'path/to/image.tif'\n",
    " \n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    "nbands = naip_ds.RasterCount\n",
    "band_data = []\n",
    "\n",
    "for i in range(1, nbands+1):\n",
    "    band = naip_ds.GetRasterBand(i).ReadAsArray()\n",
    "    band_data.append(band)\n",
    "band_data = np.dstack(band_data)\n",
    "img = exposure.rescale_intensity(band_data)\n",
    "\n",
    "# do segmentation, different options with quickshift and slic (only use one of the next two lines)\n",
    "segments = quickshift(img, ratio=0.99, max_dist=5, convert2lab=False)\n",
    "segments = slic(img, n_segments=500000, compactness=0.1)\n",
    "print('segments complete', time.time() - seg_start)\n",
    " \n",
    "# save segments to raster\n",
    "segments_fn = 'path/to/segments.tif'\n",
    "segments_ds = driverTiff.Create(segments_fn, naip_ds.RasterXSize, naip_ds.RasterYSize,\n",
    "                                1, gdal.GDT_Float32)\n",
    "segments_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "segments_ds.SetProjection(naip_ds.GetProjectionRef())\n",
    "segments_ds.GetRasterBand(1).WriteArray(segments)\n",
    "segments_ds = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images below demonstrate how the segmentation algorithm and parameters affect the size and shape of the image segments. Segments in the first image (quickshift) don’t capture the features in the image very well. You see segments overlapping roads, fields, and forested areas. In the second image, the segments from the SLIC algorithm do a much better following the boundaries of image features. The remainder of this article uses the SLIC result. Features in your image may be best represented by a different algorithm, or different algorithm parameters. Be sure to assess your segments before continuing with classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Properties of Image Segments\n",
    "\n",
    "Once the image is segmented the spectral properties of each segment must be quantitatively described. Given a number of pixels, the function below calculates descriptive statistics (e.g. mean, max, min, variance) for each band. These are the values that will be used by the random forests algorithm to classify the segments into land cover types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features(segment_pixels):\n",
    "    features = []\n",
    "    npixels, nbands = segment_pixels.shape\n",
    "    for b in range(nbands):\n",
    "        stats = scipy.stats.describe(segment_pixels[:, b])\n",
    "        band_stats = list(stats.minmax) + list(stats)[2:]\n",
    "        if npixels == 1:\n",
    "            # in this case the variance = nan, change it 0.0\n",
    "            band_stats[3] = 0.0\n",
    "        features += band_stats\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we loop through each segment, send the pixels from each segment to the `segment_features` function and save the results in a list. This section of code will likely be the bottleneck in your processing time. It can be improved with parallelization, but that isn’t discussed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids = np.unique(segments)\n",
    "objects = []\n",
    "object_ids = []\n",
    "for id in segment_ids:\n",
    "    segment_pixels = img[segments == id]\n",
    "    object_features = segment_features(segment_pixels)\n",
    "    objects.append(object_features)\n",
    "    object_ids.append(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truth (Training and Test) Data\n",
    "\n",
    "This is a supervised classification workflow, so you’ll need to have some truth data describing the land cover types represented in your classification. I quickly generated the points below in QGIS to represent seven different land cover classes. These data are just an example. Ideally, you would have data collected in a more organized and statistically rigorous manner.\n",
    "\n",
    "The land cover truth data need to be split into training and test data sets. The training data set will train the random forests classification algorithm. We will compare the classification results to the test data set to assess classification accuracy.\n",
    "\n",
    "My land cover data are in shapefile format. The code below uses `geopandas` to read the truth data as a geodataframe. Randomly, 70% of the truth observations are assigned to a training data set and the remaining 30% to a testing data set. The training and test data sets are each saved to a new shapefile. During this process I also used a lookup table that I created to give names to each land cover class (lines 8–11). This is not necessary, but makes it easier to see what each class represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read shapefile to geopandas geodataframe\n",
    "gdf = gpd.read_file('path/to/truth_data.shp')\n",
    "# get names of land cover classes/labels\n",
    "class_names = gdf['label'].unique()\n",
    "# create a unique id (integer) for each land cover class/label\n",
    "class_ids = np.arange(class_names.size) + 1\n",
    "# create a pandas data frame of the labels and ids and save to csv\n",
    "df = pd.DataFrame({'label': class_names, 'id': class_ids})\n",
    "df.to_csv('C:/temp/naip/class_lookup.csv')\n",
    "# add a new column to geodatafame with the id for each class/label\n",
    "gdf['id'] = gdf['label'].map(dict(zip(class_names, class_ids)))\n",
    " \n",
    "# split the truth data into training and test data sets and save each to a new shapefile\n",
    "gdf_train = gdf.sample(frac=0.7)  # 70% of observations assigned to training data (30% to test data)\n",
    "gdf_test = gdf.drop(gdf_train.index)\n",
    "# save training and test data to shapefiles\n",
    "gdf_train.to_file('path/to/save/train_data.shp')\n",
    "gdf_test.to_file('path/to/save/test_data.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert the training data to raster format so each observation point can be associated with an image segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = 'path/to/train_data.shp'\n",
    "train_ds = ogr.Open(train_fn)\n",
    "lyr = train_ds.GetLayer()\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "# retrieve the rasterized data and print basic stats\n",
    "data = target_ds.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate each training observation with the corresponding image segment. Lines 13–19 ensure that each training observation is associated with only one segment. Because segments include multiple pixels, it is possible that segments represent multiple land cover types. This is why it is important to properly tune your segmentation algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterized observation (truth, training and test) data\n",
    "ground_truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# get unique values (0 is the background, or no data, value so it is not included) for each land cover type\n",
    "classes = np.unique(ground_truth)[1:]\n",
    "\n",
    "# for each class (land cover type) record the associated segment IDs\n",
    "segments_per_class = {}\n",
    "for klass in classes:\n",
    "    segments_of_class = segments[ground_truth == klass]\n",
    "    segments_per_class[klass] = set(segments_of_class)\n",
    " \n",
    "# make sure no segment ID represents more than one class\n",
    "intersection = set()\n",
    "accum = set()\n",
    "for class_segments in segments_per_class.values():\n",
    "    intersection |= accum.intersection(class_segments)\n",
    "    accum |= class_segments\n",
    "assert len(intersection) == 0, \"Segment(s) represent multiple classes\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Land Cover Classification\n",
    "\n",
    "This is the meat of the analysis. The classification algorithm. First, identify and label the training objects (lines 1–20). This process involves associating a label (land cover type) with the statistics describing each spectral band within the image segment.\n",
    "\n",
    "Now, everything is now set up to train a classifier and use it to predict across all segments in the image. Here I’m using random forests, a popular classification algorithm. The code to train (fit) the algorithm and make predictions is quite simple (lines 22–24). Simply pass the training objects (containing the spectral properties) and the associated land cover label to the classifier. Once the classifier is trained (fitted) predictions can be made for non-training segments based on their spectral properties. After the predictions are made, save them to raster for display in a GIS (lines 26–43)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.copy(segments)\n",
    "threshold = train_img.max() + 1  # make the threshold value greater than any land cover class value\n",
    "\n",
    "# all pixels in training segments assigned value greater than threshold\n",
    "for klass in classes:\n",
    "    class_label = threshold + klass\n",
    "    for segment_id in segments_per_class[klass]:\n",
    "        train_img[train_img == segment_id] = class_label\n",
    " \n",
    "# training segments receive land cover class value, all other segments 0\n",
    "train_img[train_img <= threshold] = 0\n",
    "train_img[train_img > threshold] -= threshold\n",
    "\n",
    "# create objects and labels for training data\n",
    "training_objects = []\n",
    "training_labels = []\n",
    "for klass in classes:\n",
    "    class_train_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class[klass]]\n",
    "    training_labels += [klass] * len(class_train_object)\n",
    "    training_objects += class_train_object\n",
    " \n",
    "classifier = RandomForestClassifier(n_jobs=-1)  # setup random forest classifier\n",
    "classifier.fit(training_objects, training_labels)  # fit rf classifier\n",
    "predicted = classifier.predict(objects)  # predict with rf classifier\n",
    "\n",
    "# create numpy array from rf classifiation and save to raster\n",
    "clf = np.copy(segments)\n",
    "for segment_id, klass in zip(segment_ids, predicted):\n",
    "    clf[clf == segment_id] = klass\n",
    " \n",
    "mask = np.sum(img, axis=2)  # this section masks no data values\n",
    "mask[mask > 0.0] = 1.0\n",
    "mask[mask == 0.0] = -1.0\n",
    "clf = np.multiply(clf, mask)\n",
    "clf[clf < 0] = -9999.0\n",
    " \n",
    "clfds = driverTiff.Create('path/to/classified_result.tif', naip_ds.RasterXSize, naip_ds.RasterYSize,\n",
    "                          1, gdal.GDT_Float32)  # this section saves to raster\n",
    "clfds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "clfds.SetProjection(naip_ds.GetProjection())\n",
    "clfds.GetRasterBand(1).SetNoDataValue(-9999.0)\n",
    "clfds.GetRasterBand(1).WriteArray(clf)\n",
    "clfds = None\n",
    " \n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Assessment with a Confusion Matrix\n",
    "\n",
    "Accuracy assessment is a crucial aspect of any classification. If your classification doesn’t represent what it’s supposed to, it’s not worth much. Because the emphasis of this article is to describe the GeOBIA workflow, I’m not presenting my own accuracy results. Instead, I’ll show how to generate a basic confusion matrix for accuracy assessment.\n",
    "\n",
    "Load the test data set created earlier and convert it to raster format so it is compatible with the generated predictions. Then simply query the predicted values from the locations where test data exist. Finally, generate the confusion matrix from the corresponding values.\n",
    "\n",
    "Note: We classified segments, but this accuracy assessment compares pixels. We’re comparing all the pixels in each test segment to all the pixels in the corresponding predicted segment. This could lead to some bias if certain land cover classes are more frequently found in smaller (or larger) segments than others. Again, if you have done due diligence with image segmentation, this shouldn’t be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gdal\n",
    "import ogr\n",
    "from sklearn import metrics\n",
    " \n",
    "# read original image to get info for raster dimensions\n",
    "naip_fn = 'path/to/image.tif'\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    " \n",
    "# rasterize test data for pixel-to-pixel comparison\n",
    "test_fn = 'path/to/test.shp'\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    " \n",
    "truth = target_ds.GetRasterBand(1).ReadAsArray()  # truth/test data array\n",
    " \n",
    "pred_ds = gdal.Open('path/to/classified_result.tif')  \n",
    "pred = pred_ds.GetRasterBand(1).ReadAsArray()  # predicted data array\n",
    "idx = np.nonzero(truth) # get indices where truth/test has data values\n",
    "cm = metrics.confusion_matrix(truth[idx], pred[idx])  # create a confusion matrix at the truth/test locations\n",
    " \n",
    "# pixel accuracy\n",
    "print(cm)\n",
    "print(cm.diagonal())\n",
    "print(cm.sum(axis=0))\n",
    "accuracy = cm.diagonal() / cm.sum(axis=0)  # overall accuracy\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6090159f195111259c09d95d79fb1f2622e138688553073b14c580eb658dcffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
