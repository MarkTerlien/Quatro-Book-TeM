{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title:  Object-based landuse classification\n",
    "author: Mark Terlien\n",
    "date: \"December 2nd, 2022\"\n",
    "format: \n",
    "  html:\n",
    "    cold-fold: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object-based landuse classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[link](https://towardsdatascience.com/object-based-land-cover-classification-with-python-cbe54e9c9e24)\n",
    "\n",
    "An in-depth explanation of GEOBIA can be found [here](https://www.mdpi.com/2072-4292/12/12/2012#). A practical example can be found [here](https://www.mdpi.com/2072-4292/6/7/6111).\n",
    "\n",
    "Aerial images cover the entire globe at various spatial and temporal resolutions. Timely extraction of information from aerial images requires automated analysis to train computers to recognize what the human eye immediately identifies. Object-based image analysis (OBIA) improves processing efficiency by implementing image segmentation algorithms to combine groups of pixels into objects (segments) reducing the amount of information in and image. This article describes how to use open source Python packages to perform image segmentation and land cover classification of an aerial image. Specifically, I will demonstrate the process of geographic object-based image analysis (GeOBIA)to perform supervised land cover classification in 5 steps:\n",
    "\n",
    "1. Image segmentation\n",
    "2. Quantify segment spectral properties\n",
    "3. Truth data\n",
    "4. Land cover classification\n",
    "5. Accuracy assessment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Python modules\n",
    "\n",
    "The first step is to import the modules we are going to use in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from scikit-image and scipy\n",
    "from skimage import exposure\n",
    "from skimage.segmentation import slic\n",
    "import scipy\n",
    "\n",
    "# Imports gdal and ogr\n",
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "from osgeo import gdalnumeric\n",
    "from osgeo import gdal_array\n",
    "\n",
    "# Import rasterio\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import shapes\n",
    "\n",
    "# Import numpy, pandas and geopandas\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image segmentation\n",
    "\n",
    "Image segmentation is a method in which a digital image is broken down into various subgroups called Image segments which helps in reducing the complexity of the image to make further processing or analysis of the image simpler. Segmentation in easy words is assigning labels to pixels. All picture elements or pixels belonging to the same category have a common label assigned to them. \n",
    "\n",
    "Segmentation effectively reduces the number of elements in an image that need to be classified. This may reduce an image with 1 million pixels down to 50,000 segments, which is much more manageable.\n",
    "\n",
    "[Here](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py) you can read more about image segmentation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the imput parameters\n",
    "\n",
    "After importing the modules, we have to define the locations and the names of the folders and files we are going to use and going to create. \n",
    "\n",
    "We are going to use a satellite image from Waalwijk. Run the following codeblock to define the input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with data files\n",
    "data_folder = 'data_files/Waalwijk/' \n",
    "sentinel_folder = data_folder + 'sentinel-2/'\n",
    "\n",
    "# Name shapefile with sample areas and name of column with landuse classes\n",
    "sample_areas_vector_file = data_folder + '/samples_areas.shp'\n",
    "landuse_column = 'landuse'\n",
    "\n",
    "# Name rasterfile with segments\n",
    "segments_raster_file = data_folder + '/segments.tif'\n",
    "\n",
    "# Name of new GeoTIFF file with all bands\n",
    "sentinel_bands_raster_file = data_folder + 'sentinel_bands.tif'\n",
    "\n",
    "# Name of new GeoTIFF file with groud truth sample areas\n",
    "sample_areas_raster_file = os.path.splitext(sample_areas_vector_file)[0] + '.tif'\n",
    "\n",
    "# Name of new Geotiff file with classified landuse classes\n",
    "classified_landuse_raster_file = data_folder + 'classified.tif'\n",
    "classified_landuse_vector_file = data_folder + 'classified.shp'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the satellite images\n",
    "\n",
    "Now we need to collect all the Sentinel-2 bands because they come as individual images one per band. We merge them into one GeoTIFF image with multiple bands and then we plot the individual bands and the a false color composite for a visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find every file in the sentinal directory\n",
    "sentinal_band_paths_array = [os.path.join(sentinel_folder, f) for f in os.listdir(sentinel_folder) if os.path.isfile(os.path.join(sentinel_folder, f))]\n",
    "sentinal_band_paths_array.sort()\n",
    "\n",
    "# Read metadata of first file and assume all other bands are the same\n",
    "with rasterio.open(sentinal_band_paths_array[0]) as sentinal_band_path_handle:\n",
    "    meta = sentinal_band_path_handle.meta\n",
    "\n",
    "# Update metadata to reflect the number of layers\n",
    "meta.update(count = len(sentinal_band_paths_array))\n",
    "\n",
    "# Read each layer and write it to stack\n",
    "with rasterio.open(sentinel_bands_raster_file, 'w', **meta) as sentinel_bands_handle:\n",
    "    for id, layer in enumerate(sentinal_band_paths_array, start=1):\n",
    "        with rasterio.open(layer) as layer_handle:\n",
    "            sentinel_bands_handle.write_band(id, layer_handle.read(1))\n",
    "\n",
    "# Close files\n",
    "sentinal_band_path_handle = None\n",
    "sentinel_bands_handle = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demonstrates segmentation with the SLIC (Simple linear iterative clustering). First, each of the 4 bands (red, blue, green, near-infrared) from the image is read as a `numpy` array with `gdal`. Band data are re-scaled to intensity values (ranging from 0â€“1). Then segments are created. Segments are saved to a new raster with `gdal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image has 1059682 pixels\n",
      "Reasonable number of segments for SLIC is estimated at 5298\n",
      "Segmentation complete\n"
     ]
    }
   ],
   "source": [
    "# Open rastefile with bands\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "sentinel_bands_handle = gdal.Open(sentinel_bands_raster_file)\n",
    "nbands = sentinel_bands_handle.RasterCount\n",
    "band_data = []\n",
    "\n",
    "# Build stack with all band arrays and rescale between 0 and 1\n",
    "for i in range(1, nbands+1):\n",
    "    band = sentinel_bands_handle.GetRasterBand(i).ReadAsArray()\n",
    "    band_data.append(band)\n",
    "band_data = np.dstack(band_data)\n",
    "img = exposure.rescale_intensity(band_data)\n",
    "\n",
    "# Estimate the number of segments\n",
    "number_of_pixels = sentinel_bands_handle.RasterXSize * sentinel_bands_handle.RasterYSize\n",
    "print('Image has ' + str(number_of_pixels) + ' pixels')\n",
    "max_number_of_segments = number_of_pixels/200\n",
    "print('Reasonable number of segments for SLIC is estimated at ' + str(int(max_number_of_segments)))\n",
    "\n",
    "# Segmentation, different options with quickshift and slic (only use one of the next two lines)\n",
    "segments = slic(img, n_segments=max_number_of_segments, compactness=0.1)\n",
    "print('Segmentation complete')\n",
    " \n",
    "# Save segments to raster\n",
    "segments_fn = segments_raster_file\n",
    "segments_ds = driverTiff.Create(segments_fn, sentinel_bands_handle.RasterXSize, sentinel_bands_handle.RasterYSize,\n",
    "                                1, gdal.GDT_Float32)\n",
    "segments_ds.SetGeoTransform(sentinel_bands_handle.GetGeoTransform())\n",
    "segments_ds.SetProjection(sentinel_bands_handle.GetProjectionRef())\n",
    "segments_ds.GetRasterBand(1).WriteArray(segments)\n",
    "segments_ds = None\n",
    "segments_fn = None\n",
    "\n",
    "# TO DO: Vectorize segments.tif \n",
    "# TO DO: Plot vectorized segments.shp on top of satellite image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segments from the SLIC algorithm follow the boundaries of image features. Be sure to assess your segments before continuing with classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of SLIC\n",
    "\n",
    "SLIC (Simple Linear Iterative Clustering) is an algorithm for Superpixel generation. A superpixel can be defined as a group of pixels that share common characteristics (like pixel intensity ). They are often used in image classification because:\n",
    "\n",
    "- They carry more information than pixels.\n",
    "- They have a perceptual meaning since pixels belonging to a given superpixel share similar visual properties.\n",
    "- They provide a compact representation of images that can be useful for computationally demanding problems like landuse classification.\n",
    "\n",
    "The SLIC algorithm generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. [Here](https://www.mdpi.com/2072-4292/9/3/243) you can find an explanation of the the SLIC algorithm.\n",
    "\n",
    "https://www.mdpi.com/2072-4292/9/3/243\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantify segment spectral properties\n",
    "\n",
    "Once the image is segmented the spectral properties of each segment must be quantitatively described. Given a number of pixels, the function below calculates descriptive statistics (e.g. mean, max, min, variance) for each band. These are the values that will be used by the random forests algorithm to classify the segments into landuse types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate stats for each segment\n",
    "def segment_features(segment_pixels):\n",
    "    features = []\n",
    "    npixels, nbands = segment_pixels.shape\n",
    "    for b in range(nbands):\n",
    "        stats = scipy.stats.describe(segment_pixels[:, b])\n",
    "        band_stats = list(stats.minmax) + list(stats)[2:]\n",
    "        if npixels == 1:\n",
    "            # In this case the variance = nan, change it 0.0\n",
    "            band_stats[3] = 0.0\n",
    "        features += band_stats\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we loop through each segment, send the pixels from each segment to the `segment_features` function and save the results in a list. See [here](https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85) for an explanation of skewness and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print statistics of last segment\n",
      "[672, 1682, 1072.9248120300751, 77400.69127363864, 0.6823140129636995, -0.7654967278597749, 963, 2514, 1705.3383458646617, 206794.14980633406, 0.5368245984773824, -0.9769800418884813, 665, 2395, 1269.9323308270677, 264070.36659831397, 0.9816413537240936, -0.6362042477840482, 5546, 18190, 12124.75939849624, 9658895.956823878, 0.46817503771224167, -0.7224128685392146]\n",
      "Number of observations  = 672\n",
      "Minimum and maximum value  = 1682\n",
      "Arithmetic mean  = 1072.9248120300751\n",
      "Unbiased variance  = 77400.69127363864\n",
      "Skewness  = 0.6823140129636995\n",
      "Skewness  = -0.7654967278597749\n"
     ]
    }
   ],
   "source": [
    "# Get the unique ID's for the segments\n",
    "segment_ids = np.unique(segments)\n",
    "\n",
    "# Init the lists \n",
    "objects = []\n",
    "object_ids = []\n",
    "\n",
    "# For each segment\n",
    "for id in segment_ids:\n",
    "\n",
    "    # Select pixels with same ID\n",
    "    segment_pixels = img[segments == id]\n",
    "\n",
    "    # Call function to calculate stats for each segment and store stats and ID's in lists\n",
    "    object_features = segment_features(segment_pixels)\n",
    "    objects.append(object_features)\n",
    "    object_ids.append(id)\n",
    "\n",
    "# Print stats for last segment\n",
    "print('Exmple of the statistics of the first band of the last segment')\n",
    "print('Minimum = ' + str(object_features[0]))\n",
    "print('Maximum = ' + str(object_features[1]))\n",
    "print('Mean  = ' + str(object_features[2]))\n",
    "print('Variance  = ' + str(object_features[3]))\n",
    "print('Skewness  = ' + str(object_features[4]))\n",
    "print('Skewness  = ' + str(object_features[5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truth (Training and Test) Data\n",
    "\n",
    "This is a supervised classification workflow, so youâ€™ll need to have some truth data describing the landuse types represented in your classification. \n",
    "\n",
    "The landuse truth data need to be split into training and test data sets. The training data set will train the random forests classification algorithm. We will compare the classification results to the test data set to assess classification accuracy.\n",
    "\n",
    "The landuse data is available as a shapefile. The code below uses `geopandas` to read the truth data as a geodataframe. Randomly, 70% of the truth observations are assigned to a training data set and the remaining 30% to a testing data set. The training and test data sets are each saved to a new shapefile. During this process I also used a lookup table that I created to give names to each land cover class (lines 8â€“11). This is not necessary, but makes it easier to see what each class represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read shapefile to geopandas geodataframe\n",
    "gdf = gpd.read_file('path/to/truth_data.shp')\n",
    "# get names of land cover classes/labels\n",
    "class_names = gdf['label'].unique()\n",
    "# create a unique id (integer) for each land cover class/label\n",
    "class_ids = np.arange(class_names.size) + 1\n",
    "# create a pandas data frame of the labels and ids and save to csv\n",
    "df = pd.DataFrame({'label': class_names, 'id': class_ids})\n",
    "df.to_csv('C:/temp/naip/class_lookup.csv')\n",
    "# add a new column to geodatafame with the id for each class/label\n",
    "gdf['id'] = gdf['label'].map(dict(zip(class_names, class_ids)))\n",
    " \n",
    "# split the truth data into training and test data sets and save each to a new shapefile\n",
    "gdf_train = gdf.sample(frac=0.7)  # 70% of observations assigned to training data (30% to test data)\n",
    "gdf_test = gdf.drop(gdf_train.index)\n",
    "# save training and test data to shapefiles\n",
    "gdf_train.to_file('path/to/save/train_data.shp')\n",
    "gdf_test.to_file('path/to/save/test_data.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert the training data to raster format so each observation point can be associated with an image segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = 'path/to/train_data.shp'\n",
    "train_ds = ogr.Open(train_fn)\n",
    "lyr = train_ds.GetLayer()\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "# retrieve the rasterized data and print basic stats\n",
    "data = target_ds.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate each training observation with the corresponding image segment. Lines 13â€“19 ensure that each training observation is associated with only one segment. Because segments include multiple pixels, it is possible that segments represent multiple land cover types. This is why it is important to properly tune your segmentation algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterized observation (truth, training and test) data\n",
    "ground_truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# get unique values (0 is the background, or no data, value so it is not included) for each land cover type\n",
    "classes = np.unique(ground_truth)[1:]\n",
    "\n",
    "# for each class (land cover type) record the associated segment IDs\n",
    "segments_per_class = {}\n",
    "for klass in classes:\n",
    "    segments_of_class = segments[ground_truth == klass]\n",
    "    segments_per_class[klass] = set(segments_of_class)\n",
    " \n",
    "# make sure no segment ID represents more than one class\n",
    "intersection = set()\n",
    "accum = set()\n",
    "for class_segments in segments_per_class.values():\n",
    "    intersection |= accum.intersection(class_segments)\n",
    "    accum |= class_segments\n",
    "assert len(intersection) == 0, \"Segment(s) represent multiple classes\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Land Cover Classification\n",
    "\n",
    "This is the meat of the analysis. The classification algorithm. First, identify and label the training objects (lines 1â€“20). This process involves associating a label (land cover type) with the statistics describing each spectral band within the image segment.\n",
    "\n",
    "Now, everything is now set up to train a classifier and use it to predict across all segments in the image. Here Iâ€™m using random forests, a popular classification algorithm. The code to train (fit) the algorithm and make predictions is quite simple (lines 22â€“24). Simply pass the training objects (containing the spectral properties) and the associated land cover label to the classifier. Once the classifier is trained (fitted) predictions can be made for non-training segments based on their spectral properties. After the predictions are made, save them to raster for display in a GIS (lines 26â€“43)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.copy(segments)\n",
    "threshold = train_img.max() + 1  # make the threshold value greater than any land cover class value\n",
    "\n",
    "# all pixels in training segments assigned value greater than threshold\n",
    "for klass in classes:\n",
    "    class_label = threshold + klass\n",
    "    for segment_id in segments_per_class[klass]:\n",
    "        train_img[train_img == segment_id] = class_label\n",
    " \n",
    "# training segments receive land cover class value, all other segments 0\n",
    "train_img[train_img <= threshold] = 0\n",
    "train_img[train_img > threshold] -= threshold\n",
    "\n",
    "# create objects and labels for training data\n",
    "training_objects = []\n",
    "training_labels = []\n",
    "for klass in classes:\n",
    "    class_train_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class[klass]]\n",
    "    training_labels += [klass] * len(class_train_object)\n",
    "    training_objects += class_train_object\n",
    " \n",
    "classifier = RandomForestClassifier(n_jobs=-1)  # setup random forest classifier\n",
    "classifier.fit(training_objects, training_labels)  # fit rf classifier\n",
    "predicted = classifier.predict(objects)  # predict with rf classifier\n",
    "\n",
    "# create numpy array from rf classifiation and save to raster\n",
    "clf = np.copy(segments)\n",
    "for segment_id, klass in zip(segment_ids, predicted):\n",
    "    clf[clf == segment_id] = klass\n",
    " \n",
    "mask = np.sum(img, axis=2)  # this section masks no data values\n",
    "mask[mask > 0.0] = 1.0\n",
    "mask[mask == 0.0] = -1.0\n",
    "clf = np.multiply(clf, mask)\n",
    "clf[clf < 0] = -9999.0\n",
    " \n",
    "clfds = driverTiff.Create('path/to/classified_result.tif', naip_ds.RasterXSize, naip_ds.RasterYSize,\n",
    "                          1, gdal.GDT_Float32)  # this section saves to raster\n",
    "clfds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "clfds.SetProjection(naip_ds.GetProjection())\n",
    "clfds.GetRasterBand(1).SetNoDataValue(-9999.0)\n",
    "clfds.GetRasterBand(1).WriteArray(clf)\n",
    "clfds = None\n",
    " \n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Assessment with a Confusion Matrix\n",
    "\n",
    "Accuracy assessment is a crucial aspect of any classification. If your classification doesnâ€™t represent what itâ€™s supposed to, itâ€™s not worth much. Because the emphasis of this article is to describe the GeOBIA workflow, Iâ€™m not presenting my own accuracy results. Instead, Iâ€™ll show how to generate a basic confusion matrix for accuracy assessment.\n",
    "\n",
    "Load the test data set created earlier and convert it to raster format so it is compatible with the generated predictions. Then simply query the predicted values from the locations where test data exist. Finally, generate the confusion matrix from the corresponding values.\n",
    "\n",
    "Note: We classified segments, but this accuracy assessment compares pixels. Weâ€™re comparing all the pixels in each test segment to all the pixels in the corresponding predicted segment. This could lead to some bias if certain land cover classes are more frequently found in smaller (or larger) segments than others. Again, if you have done due diligence with image segmentation, this shouldnâ€™t be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gdal\n",
    "import ogr\n",
    "from sklearn import metrics\n",
    " \n",
    "# read original image to get info for raster dimensions\n",
    "naip_fn = 'path/to/image.tif'\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    " \n",
    "# rasterize test data for pixel-to-pixel comparison\n",
    "test_fn = 'path/to/test.shp'\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    " \n",
    "truth = target_ds.GetRasterBand(1).ReadAsArray()  # truth/test data array\n",
    " \n",
    "pred_ds = gdal.Open('path/to/classified_result.tif')  \n",
    "pred = pred_ds.GetRasterBand(1).ReadAsArray()  # predicted data array\n",
    "idx = np.nonzero(truth) # get indices where truth/test has data values\n",
    "cm = metrics.confusion_matrix(truth[idx], pred[idx])  # create a confusion matrix at the truth/test locations\n",
    " \n",
    "# pixel accuracy\n",
    "print(cm)\n",
    "print(cm.diagonal())\n",
    "print(cm.sum(axis=0))\n",
    "accuracy = cm.diagonal() / cm.sum(axis=0)  # overall accuracy\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6090159f195111259c09d95d79fb1f2622e138688553073b14c580eb658dcffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
